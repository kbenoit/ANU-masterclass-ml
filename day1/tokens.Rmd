---
title: "Creating and working with tokens objects"
output: github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "images/"
)
```

```{r}
library("quanteda")
library("quanteda.textmodels")
```


## Tokenization of a corpus

```{r}
corp_immig <- corpus(data_char_ukimmig2010)
toks_immig <- tokens(corp_immig)
print(toks_immig, max_ntoken = 5, max_ndoc = 3)
```

### Exercise

1. Tokenize while removing punctuation, and compare.

```{r}

```

2. Tokenize while removing symbols.  Does this make any visible difference?  Try repeating the command with `verbose = TRUE`.

```{r}

```

3. Tokenize while removing numbers, and compare as you did before.

```{r}

```


## Removing stopwords

### Exercise

1. Remove stopwords from the tokens, after tokenization.

```{r}

```

2. Remove them, but with `padding = TRUE`.  What difference does this make, and when might you want to use this option?

```{r}

```

3. Try removing the following "custom" stopwords: variations of "immigrant".

```{r}

```

4. Try removing stopwords from another language.  How would you do this?


```{r}

```


## Selecting certain terms

```{r}
toks_immig_select <- tokens_keep(toks_immig,
                                 pattern = c("immig*", "migra*"),
                                 padding = TRUE)
print(toks_immig_select, max_ntoken = 5, max_ndoc = 3)
```

### Exercise

Repeat the above but without `padding = TRUE`.  What is the difference?  When might you want to use padding?

```{r}

```

## Select certain terms and their context

```{r}
# specify number of surrounding words using window
toks_immig_window <- tokens_keep(toks_immig, 
                                   pattern = c('immig*', 'migra*'),
                                   padding = TRUE, window = 5)
print(toks_immig_window, max_ntoken = 20, max_ndoc = 3)
```



## Compound multiword expressions

```{r}
kw_multi <- kwic(data_char_ukimmig2010, 
                 phrase(c('asylum seeker*', 'british citizen*')))
head(kw_multi, 5)
```

### Exercise

Why was the `phrase()` function called, and what does it do?



## Compound multiword expressions

Preserve these expressions in bag-of-word analysis: 

```{r}
toks_comp <- tokens(data_char_ukimmig2010) %>% 
  tokens_compound(phrase(c('asylum seeker*', 'british citizen*')))

kw_comp <- kwic(toks_comp, c('asylum_seeker*', 'british_citizen*'))
head(kw_comp, 4)
```


## Exercise

1. Tokenize `data_corpus_irishbudget2010` and compound the following party names: `fianna fáil`, `fine gael`, and `sinn féin`.

```{r}

```

2. Select only the three party names and the window of +-10 words.

```{r}

```

3. How can we extract only the full _sentences_ in which at least one of the parties is mentioned?

```{r}

```


---


## N-grams

```{r}
# unigrams
tokens("insurgents killed in ongoing fighting")

# bigrams
tokens("insurgents killed in ongoing fighting") %>% 
    tokens_ngrams(n = 2)

# skip-grams
tokens("insurgents killed in ongoing fighting") %>% 
    tokens_skipgrams(n = 2, skip = 0:1)
```


## Look up tokens from a dictionary

```{r}
toks <- tokens(data_char_ukimmig2010)

dict <- dictionary(list(refugee = c('refugee*', 'asylum*'),
                        worker = c('worker*', 'employee*')))
print(dict)

dict_toks <- tokens_lookup(toks, dictionary = dict)
head(dict_toks, 2)
```



## The transition from tokens() to dfm()

```{r}
dfm(dict_toks)
```


### Exercise

1. Tokenize `data_corpus_irishbudget2010`.

```{r}

```

2. Convert the tokens object, remove punctuation, change to lowercase, remove stopwords, and stem.

```{r}

```

3. Get the number of types and tokens per speech.

```{r}

```

4. Compare the types and tokens based on the tokens object to `ntype(data_corpus_irishbudget2010)` and `ntoken(data_corpus_irishbudget2010)`. How and why do the values differ? 
```{r}

```


